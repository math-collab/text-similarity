{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12fca215-77e3-4e56-a506-c778e5cca40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3042020/1524277177.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange\n",
      "2023-05-28 18:41:26.660379: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from typing import List, Union\n",
    "import math\n",
    "from tqdm.autonotebook import trange\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from sentence_transformers.evaluation import SentenceEvaluator\n",
    "from sentence_transformers import InputExample\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class LossEvaluator(SentenceEvaluator):\n",
    "\n",
    "    def __init__(self, loader, device: str = \"cpu\", loss_model: nn.Module = None, name: str = '', log_dir: str = None,\n",
    "                 show_progress_bar: bool = False, write_csv: bool = True):\n",
    "\n",
    "        \"\"\"\n",
    "        Evaluate a model based on the loss function.\n",
    "        The returned score is loss value.\n",
    "        The results are written in a CSV and Tensorboard logs.\n",
    "        :param loader: Data loader object\n",
    "        :param loss_model: loss module object\n",
    "        :param name: Name for the output\n",
    "        :param log_dir: path for tensorboard logs \n",
    "        :param show_progress_bar: If true, prints a progress bar\n",
    "        :param write_csv: Write results to a CSV file\n",
    "        \"\"\"\n",
    "\n",
    "        self.loader = loader\n",
    "        self.write_csv = write_csv\n",
    "        self.logs_writer = SummaryWriter(log_dir=log_dir)\n",
    "        self.name = name\n",
    "        self.loss_model = loss_model\n",
    "        \n",
    "        # move model to gpu:  lidija-jovanovska\n",
    "        self.device = device #torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        loss_model.to(self.device)\n",
    "\n",
    "        if show_progress_bar is None:\n",
    "            show_progress_bar = (\n",
    "                    logger.getEffectiveLevel() == logging.INFO or logger.getEffectiveLevel() == logging.DEBUG)\n",
    "        self.show_progress_bar = show_progress_bar\n",
    "\n",
    "        self.csv_file: str = \"loss_evaluation\" + (\"_\" + name if name else '') + \"_results.csv\"\n",
    "        #self.csv_file: str = \"triplet_evaluation\" + (\"_\" + name if name else \"\") + \"_results.csv\"\n",
    "        self.csv_headers = [\"epoch\", \"steps\", \"loss\"]\n",
    "    \n",
    "    \n",
    "    def __call__(self, model, output_path: str = None, epoch: int = -1, steps: int = -1) -> float:\n",
    "        if epoch != -1:\n",
    "            if steps == -1:\n",
    "                out_txt = \" after epoch {}:\".format(epoch)\n",
    "            else:\n",
    "                out_txt = \" in epoch {} after {} steps:\".format(epoch, steps)\n",
    "        else:\n",
    "            out_txt = \":\"\n",
    "            \n",
    "        logger.info(\"LossEvaluator: Evaluating the model on \" + self.name + \" dataset\" + out_txt)\n",
    "        self.loss_model.eval()\n",
    "\n",
    "        loss_value = 0\n",
    "        self.loader.collate_fn = model.smart_batching_collate\n",
    "        num_batches = len(self.loader)\n",
    "        data_iterator = iter(self.loader)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _ in trange(num_batches, desc=\"Iteration\", smoothing=0.05, disable=not self.show_progress_bar):\n",
    "                sentence_features, labels = next(data_iterator)\n",
    "                #move data to GPU: lidija-jovanovska\n",
    "                for i in range(0, len(sentence_features)):\n",
    "                    for key, value in sentence_features[i].items():\n",
    "                        sentence_features[i][key] = sentence_features[i][key].to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                loss_value += self.loss_model(sentence_features, labels).item()\n",
    "                #loss_value += self.loss_model(sentence_features).item()\n",
    "\n",
    "        final_loss = loss_value / num_batches\n",
    "        if output_path is not None and self.write_csv:\n",
    "\n",
    "            csv_path = os.path.join(output_path, self.csv_file)\n",
    "            output_file_exists = os.path.isfile(csv_path)\n",
    "            with open(csv_path, newline='', mode=\"a\" if output_file_exists else 'w', encoding=\"utf-8\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                if not output_file_exists:\n",
    "                    writer.writerow(self.csv_headers)\n",
    "\n",
    "                writer.writerow([epoch, steps, final_loss])\n",
    "            \n",
    "            logger.info(\"Validation loss:\\t{:.2f}\".format(final_loss))\n",
    "\n",
    "            # ...log the running loss\n",
    "            self.logs_writer.add_scalar('val_loss',\n",
    "                                        final_loss,\n",
    "                                        steps)\n",
    "\n",
    "        self.loss_model.zero_grad()\n",
    "        self.loss_model.train()\n",
    "\n",
    "        return final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fdcca4a-9392-4223-9da5-e24ccb5d7d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import simi\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler\n",
    "from sentence_transformers import models, util, datasets, evaluation, losses\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "EMBEDDING = models.Transformer('bert-base-uncased')\n",
    "POOLING = models.Pooling(EMBEDDING.get_word_embedding_dimension())) # MEAN-pooling\n",
    "# POOLING = models.Pooling(EMBEDDING.get_word_embedding_dimension(), pooling_mode=\"cls\")) # CLS-pooling\n",
    "\n",
    "MODEL = SentenceTransformer(modules=[EMBEDDING, POOLING])\n",
    "\n",
    "RANDOM_STATE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "239e52d3-b651-4c6d-954b-33b84a2f8273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>doi</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>431987</th>\n",
       "      <td>(cs.LG, eess.SP, stat.ML)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New Perspectives on the Use of Online Learning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382432</th>\n",
       "      <td>(math.NT,)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Some Results on Linearized Trinomials that Spl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598035</th>\n",
       "      <td>(math.AG, math.RA)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Differential Operators on Azumaya algebra and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254524</th>\n",
       "      <td>(stat.ME,)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Some comments about \"Penalising model componen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55515</th>\n",
       "      <td>(math.NT,)</td>\n",
       "      <td>10.1142/S1793042110003654</td>\n",
       "      <td>Construction of Self-Dual Integral Normal Base...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       categories                        doi  \\\n",
       "431987  (cs.LG, eess.SP, stat.ML)                        NaN   \n",
       "382432                 (math.NT,)                        NaN   \n",
       "598035         (math.AG, math.RA)                        NaN   \n",
       "254524                 (stat.ME,)                        NaN   \n",
       "55515                  (math.NT,)  10.1142/S1793042110003654   \n",
       "\n",
       "                                                     text  \n",
       "431987  New Perspectives on the Use of Online Learning...  \n",
       "382432  Some Results on Linearized Trinomials that Spl...  \n",
       "598035  Differential Operators on Azumaya algebra and ...  \n",
       "254524  Some comments about \"Penalising model componen...  \n",
       "55515   Construction of Self-Dual Integral Normal Base...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"abstracts-arxiv-dataset.csv\", index_col=0)\n",
    "df[\"categories\"] = df[\"categories\"].map(lambda c: tuple(c.split()))\n",
    "df = df[df[\"categories\"].map(lambda c: len(tuple(filter(lambda s: s.startswith(\"math\") or s.startswith(\"stat\"), c)))>0)]\n",
    "df = df.reset_index(drop=True)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0bd94c1-7008-4e0e-91e0-ee609ffc6454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 520388 test: 130097\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = train_test_split(df, train_size=0.8, random_state=RANDOM_STATE)\n",
    "print(\"train:\", len(X_train), \"test:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b1c6169-b911-49c0-bfd5-4989fedbe2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "When tie_encoder_decoder=True, the decoder_name_or_path will be invalid.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.query.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following encoder weights were not tied to the decoder ['bert/pooler']\n"
     ]
    }
   ],
   "source": [
    "retrain_dataset = datasets.DenoisingAutoEncoderDataset(X_train[\"text\"].reset_index(drop=True))\n",
    "retrain_dataloader = DataLoader(retrain_dataset, shuffle=True, batch_size=8)\n",
    "retrain_loss = losses.DenoisingAutoEncoderLoss(MODEL, decoder_name_or_path='bert-base-uncased', tie_encoder_decoder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec5769e3-5cca-4c3b-ac55-9ba95dee5be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = datasets.DenoisingAutoEncoderDataset(X_test[\"text\"].reset_index(drop=True))\n",
    "eval_dataloader = DataLoader(eval_dataset, shuffle=True, batch_size=8)\n",
    "evaluator = LossEvaluator(eval_dataloader, loss_model=retrain_loss, device=\"cuda:0\", show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005d0bb7-872f-49c8-8479-51139940782e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082d57e20dbb4d35bdf049413416aabb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "babf0794b6b04e6da039de5b53ca6bf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/65049 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call the fit method\n",
    "MODEL.fit(\n",
    "    train_objectives=[(retrain_dataloader, retrain_loss)],\n",
    "    evaluator=evaluator,\n",
    "    evaluation_steps=10000,\n",
    "    save_best_model=True,\n",
    "    output_path='bert+mean-pooling+re-train_tsdae_abstracts_arxiv',\n",
    "    epochs=3,\n",
    "    weight_decay=0,\n",
    "    scheduler='constantlr',\n",
    "    optimizer_params={'lr': 3e-5},\n",
    "    show_progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6794b469-0fcd-43dc-a663-ee309f0ba035",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL.save('bert+mean-pooling+re-train_tsdae_abstracts_arxiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd47b3ab-2482-4708-9ca4-18903374b534",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL.evaluate(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcf6064-7998-489a-aec2-e97317426a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd13061-4b84-4708-b470-bb59aeafd234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49e1645-13e1-4263-8494-c63f2bf81fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946c06bd-39ab-41e8-9f5a-8f1357287b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75da6e5-03da-4c39-bfdc-80f7ebd76024",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (Cuda 12.1, Tensorflow 2.16)",
   "language": "python",
   "name": "py3.10-cuda12.1-tf2.16"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
